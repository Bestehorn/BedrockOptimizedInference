{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6476a48e-3b7c-4d11-903c-b24af363d971",
   "metadata": {},
   "source": [
    "# Bedrock Optimized Latency Test (by Markus Bestehorn)\n",
    "This notebook contains code to test the optimized performance inference feature of Bedrock that has been released for public preview at re:invent 2024: https://aws.amazon.com/about-aws/whats-new/2024/12/latency-optimized-inference-foundation-models-amazon-bedrock/\n",
    "\n",
    "The code allows comparing the inference result times of the standard inference with the inference times for the latency optimized inference. **Running this test will incur costs on the AWS account running it**. For further information on the cost - particularly on optimized inference - refer to the official pricing documentation page of Amazon Bedrock: https://aws.amazon.com/bedrock/pricing/  \n",
    "\n",
    "**Disclaimer**: The code in this notebook has been written for the sole purpose of testing the aforementioned feature of Amazon Bedrock. This code is not production ready or usable for other purposes.\n",
    "\n",
    "**Prerequisites**: \n",
    "1. It is assumed here, that the this notebook runs inside a security context that has adequate priviledges to use the converse API of Amazon Bedrock.\n",
    "2. It is assumed that model access has been adequately configured in Amazon Bedrock through the \"Model Access\" page for the region that will be used (\"us-east-2\") as well as all foundational models that will be used.\n",
    "\n",
    "As a first step, we need to make sure that the most recent version of the boto3 library is installed where this notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f2ff8e-8b87-4abf-bdf3-bbfcdc87b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.19.0 requires botocore<1.36.4,>=1.36.0, but you have botocore 1.37.10 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current boto3 version: 1.37.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def update_boto3():\n",
    "    # Implement pip upgrade using subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"-q\", \"boto3\"])\n",
    "\n",
    "    # Verify the new version\n",
    "    import boto3\n",
    "    print(f\"Current boto3 version: {boto3.__version__}\")\n",
    "\n",
    "\n",
    "update_boto3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ca3d0-d8b5-4805-8027-0487d8e8dcbc",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "The test uses the Bedrock converse API to respond to the same prompt multiple times and then computes aggregates over the times required to complete these calls. The main parameters for this test are as follows:\n",
    " -  `prompt`: This is the prompt that is being sent to the Bedrock converse API. This can be modified freely and depending on the complexity of the prompt, the absolute values of the test may vary.\n",
    " -  `DEFAULT_MODEL_ID`: This is the model ID or inference profile that is used for the test. As optimized inference is currently only supported by a limited number of foundational models in Bedrock, it is not recommended to change this value.\n",
    " -  `DEFAULT_REGION`: This is the AWS region where the inference will be running. Do not change this, as optimized performance is currently not supported in other AWS regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b508f40-004b-4290-9d0b-d6b968694f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some predefined prompts that result in a different number of output tokens and therefore different processing times in Bedrock\n",
    "simple_prompt = \"\"\"Explain in a few sentences why objects cannot travel faster than the speed of light.\"\"\"\n",
    "middle_prompt = \"\"\"Write 2 paragraphs on why objects cannot travel faster than the speed of light.\"\"\"\n",
    "complex_prompt = \"\"\"Write a book chapter on why objects cannot travel faster than the speed of light.\"\"\"\n",
    "\n",
    "text_for_input_prompt_lengthening = \"\"\"\n",
    "The Ultimate Barrier: Why Objects Cannot Travel Faster Than Light\n",
    "\n",
    "The universe has many wonders, but perhaps none is more profound than the existence of an ultimate speed limit. At approximately 299,792,458 meters per second, the speed of light in vacuum represents not merely a practical barrier but a fundamental limit woven into the very fabric of spacetime. This chapter explores why material objects cannot breach this cosmic speed limit, delving into both the theoretical framework established by Einstein and the experimental evidence that supports it.\n",
    "\n",
    "The Emergence of a Universal Speed Limit\n",
    "\n",
    "Before Einstein's revolutionary work in 1905, most physicists believed that speeds could add without restriction. If you run forward at 5 km/h on a train moving at 50 km/h, your total speed relative to the ground would be 55 km/h—a straightforward addition. This Galilean view of relativity suggested no inherent limit to how fast an object might travel.\n",
    "\n",
    "Einstein's special relativity, however, revealed that this simple addition of velocities fails when speeds approach that of light. His equations showed that velocities combine according to the formula:\n",
    "\n",
    "$v_{total} = \\frac{v_1 + v_2}{1 + \\frac{v_1 v_2}{c^2}}$\n",
    "\n",
    "This elegant formula ensures that regardless of how many velocity boosts are applied, the resulting speed never exceeds c, the speed of light in vacuum.\n",
    "\n",
    "The Mass-Energy Relationship\n",
    "\n",
    "Perhaps the most famous equation in physics, $E = mc^2$, demonstrates the profound relationship between mass and energy. Less discussed, but equally important, is how this relationship affects objects approaching the speed of light.\n",
    "\n",
    "As an object with rest mass accelerates, its relativistic mass increases according to:\n",
    "\n",
    "$m = \\frac{m_0}{\\sqrt{1-\\frac{v^2}{c^2}}}$\n",
    "\n",
    "This equation reveals something remarkable: as velocity approaches the speed of light, the denominator approaches zero, causing the relativistic mass to approach infinity. Consequently, the energy required to accelerate the object further also approaches infinity.\n",
    "\n",
    "The Energy Problem\n",
    "\n",
    "Consider a spacecraft with a rest mass of 1,000 kg. Accelerating this craft to 50% the speed of light would require tremendous energy, but remains theoretically possible. At 90% light speed, the energy requirements grow dramatically. At 99%, they become staggering. But to reach precisely the speed of light? The mathematics is unequivocal: it would require infinite energy—an insurmountable barrier.\n",
    "\n",
    "This isn't merely an engineering challenge to overcome with better technology; it represents a fundamental limit imposed by the structure of reality itself.\n",
    "\n",
    "Spacetime and Causality\n",
    "\n",
    "Beyond the energy considerations lies something even more profound: the nature of spacetime itself. Special relativity reveals that space and time are not separate entities but aspects of a unified spacetime. As objects approach light speed, they experience time dilation and length contraction. At the speed of light itself, time would stop entirely from the perspective of the moving object, and its length in the direction of motion would contract to zero—physically impossible conditions for any object with mass.\n",
    "\n",
    "Furthermore, faster-than-light travel would violate causality—the principle that causes precede effects. An object moving faster than light could, in some reference frames, appear to arrive before it departed, creating paradoxes that unravel the coherent fabric of physical law.\n",
    "\n",
    "What About Quantum \"Spookiness\"?\n",
    "\n",
    "Some might point to quantum entanglement, where information seems to travel instantaneously between entangled particles, as a counterexample. However, careful analysis shows that no usable information can be transmitted faster than light through entanglement. The \"spooky action at a distance\" that troubled Einstein does not violate the cosmic speed limit.\n",
    "\n",
    "Tachyons: Theoretical Faster-Than-Light Particles\n",
    "\n",
    "Theoretical physics has explored the mathematical possibility of tachyons—hypothetical particles that always travel faster than light. Interestingly, these would need to have imaginary rest mass and could never slow down to light speed or below. Despite decades of searching, no experimental evidence supports their existence, and most physicists consider them mathematical curiosities rather than physical realities.\n",
    "\n",
    "Experimental Confirmation\n",
    "\n",
    "The universal speed limit has been tested repeatedly in particle accelerators. Protons in the Large Hadron Collider are accelerated to 99.9999991% the speed of light, requiring enormous energy input. Despite the tremendous energies involved, these particles never reach or exceed light speed, precisely as Einstein's equations predict.\n",
    "\n",
    "Loopholes and Workarounds?\n",
    "\n",
    "Science fiction often depicts \"warp drives,\" \"hyperspace,\" or \"wormholes\" as methods for effective faster-than-light travel. These concepts don't actually violate special relativity because they involve manipulating spacetime itself rather than exceeding light speed locally.\n",
    "\n",
    "For instance, the Alcubierre warp drive, a theoretical solution to Einstein's field equations, would contract spacetime in front of a vessel and expand it behind, potentially allowing effective faster-than-light travel without the vessel itself ever exceeding light speed in its local region of spacetime. However, such mechanisms require exotic matter with negative energy density—something not known to exist in sufficient quantities, if at all.\n",
    "\n",
    "Conclusion: The Profound Implications\n",
    "\n",
    "The universal speed limit is not merely a curious fact but a profound feature of our universe with far-reaching implications. It establishes ultimate horizons for our potential exploration of the cosmos. It ensures the preservation of causality. And perhaps most importantly, it reminds us that the universe operates according to deep principles that cannot be circumvented by mere technological advancement.\n",
    "\n",
    "The speed of light stands as a humbling reminder that despite our remarkable scientific progress, we remain subject to the fundamental laws that govern reality. Like the laws of thermodynamics or the uncertainty principle, the cosmic speed limit represents not a challenge to overcome, but a fundamental characteristic of the universe we inhabit—a universe more strange, more beautiful, and more constrained than our intuitions might suggest.\n",
    "\"\"\"\n",
    "\n",
    "# The variable that actually defined which prompt is being used\n",
    "prompt = complex_prompt\n",
    "DEFAULT_NO_ITERATIONS = 25\n",
    "DEFAULT_REGION = \"us-east-2\" # do not change this value => optimized performance is currently only available in us-east-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fff82-3540-4427-bbbc-b656d288560d",
   "metadata": {},
   "source": [
    "## Foundational Model\n",
    "Optimized performance is currently only supported for a limited number of foundational models as documented here: https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html\n",
    "\n",
    "The cell below configures the model ID that will be used in a variable called `DEFAULT_MODEL_ID`. If you want to change the model that is used here, use one of the other provided Model IDs and copy them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c28681-69ab-4eb5-9603-f9d0c5cc8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID_ANTHROPIC_CLAUDE3_5_HAIKU_CRIS = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "MODEL_ID_META_LLAMA_3_1_70B_INSTRUCT = \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "MODEL_ID_META_LLAMA_3_1_405B_INSTRUCT = \"us.meta.llama3-1-405b-instruct-v1:0\"\n",
    "MODEL_ID_AMAZON_NOVA_PRO_CRIS = \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL_ID = MODEL_ID_AMAZON_NOVA_PRO_CRIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33c472-1575-4254-ade2-69e388354530",
   "metadata": {},
   "source": [
    "## Code\n",
    "The following cell contains all code that is used for running the test. Note that executing the cell does not actually execute the code, but merely loads it so that it can be executed when needed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5023c053-8092-43fa-8664-9353e7ef4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from botocore.config import Config\n",
    "from datetime import datetime, timedelta\n",
    "from botocore.exceptions import ClientError, ParamValidationError\n",
    "import statistics\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "BEDROCK_THROTTLING_PAUSE = 30\n",
    "\n",
    "\n",
    "def json_to_str(input_json: dict) -> str:\n",
    "    return json.dumps(input_json, indent=3)\n",
    "\n",
    "\n",
    "def send_prompt_to_bedrock(prompt: str,\n",
    "                          additional_files: list[dict] = [],\n",
    "                          model_id: str = DEFAULT_MODEL_ID, \n",
    "                          max_tokens: int = 4096,\n",
    "                          temperature: float = 0.7,\n",
    "                          top_p: float = 0.9,\n",
    "                          api_key: str = None, \n",
    "                          region_name: str = DEFAULT_REGION,\n",
    "                          latency_optimized=False) -> (dict, float):\n",
    "    config = Config(\n",
    "        read_timeout=240,  # Timeout in seconds\n",
    "        connect_timeout=60,  # Connection timeout\n",
    "        retries={'max_attempts': 3}  # Optional: Configure retry behavior\n",
    "    )\n",
    "\n",
    "    # Create a Bedrock client\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime', region_name=region_name, config=config)\n",
    "\n",
    "    if prompt is None or len(prompt) == 0:\n",
    "        logger.error(\"Prompt cannot be empty.\")\n",
    "        return None\n",
    "\n",
    "    content_blocks = [\n",
    "        {\"text\": prompt}\n",
    "    ]\n",
    "\n",
    "    for current_doc_element in additional_files:\n",
    "        content_blocks.append({\"document\": current_doc_element})\n",
    "\n",
    "    # setup the performance config\n",
    "    performanceConfig = {}\n",
    "    if latency_optimized:\n",
    "        performanceConfig[\"latency\"] = \"optimized\"\n",
    "    else:\n",
    "        performanceConfig[\"latency\"] = \"standard\"\n",
    "    \n",
    "    # we loop here to retry in case of throttling\n",
    "    done = False\n",
    "    tries = 0\n",
    "    last_toggle_was_region = False\n",
    "    while not done:\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            tries += 1\n",
    "            logger.debug(f\"Sending prompt with the following performance config to Bedrock:\\n{json_to_str(performanceConfig)}\")\n",
    "            start_time = datetime.now()\n",
    "            response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content_blocks\n",
    "                    }\n",
    "                ],\n",
    "                inferenceConfig={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"topP\": top_p,\n",
    "                    \"maxTokens\": max_tokens\n",
    "                },\n",
    "                performanceConfig=performanceConfig\n",
    "            )\n",
    "            end_time = datetime.now()\n",
    "            elapsed_time = end_time - start_time\n",
    "            logger.debug(f\"Measured time: {elapsed_time} (Type: {elapsed_time.__class__})\")\n",
    "            total_tokens = int(response[\"usage\"][\"totalTokens\"])\n",
    "            done = True\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"ThrottlingException\":\n",
    "                pause = BEDROCK_THROTTLING_PAUSE * tries\n",
    "                logger.debug(f\"Call to Bedrock was throttled. Waiting for {pause} seconds before retrying:\\n{str(e)}\")\n",
    "                time.sleep(pause)\n",
    "                continue\n",
    "            elif e.response['Error']['Code'] == \"ServiceUnavailableException\":\n",
    "                pause = BEDROCK_THROTTLING_PAUSE * tries\n",
    "                logger.debug(f\"Bedrock services is unavailable at the moment. Waiting for {pause} seconds before retrying:\\n{str(e)}\")\n",
    "                time.sleep(pause)\n",
    "                continue\n",
    "            else:\n",
    "                logger.error(f\"Can't invoke {model_id} in {region_name} due to Client Error. Reason: {str(e)}\\nType: {str(e.__class__)}\")\n",
    "                break\n",
    "        except ParamValidationError as pe:\n",
    "            logger.error(f\"Cannot invoke model with ID {model_id} in {region_name} as the format of the parameters in the input is illegal:\\n{str(pe)}\\nError Type: {e.__class__}\\nAborting call to bedrock.\")\n",
    "            break\n",
    "        except TypeError as te:\n",
    "            logger.error(f\"Cannot invoke model with ID {model_id} in {region_name} as the call to the converse API is malformatted/illegal:\\n{str(te)}\\nError Type: {te.__class__}\\nAborting call to bedrock.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Call to Bedrock caused an Exception: {str(e)} \\nType: {str(e.__class__)}\")\n",
    "            break\n",
    "\n",
    "    logger.debug(f\"Received the following response from Bedrock:\\n{json_to_str(response)}\\nReponse Type: {response.__class__}\")\n",
    "\n",
    "    return response, elapsed_time, total_tokens\n",
    "\n",
    "\n",
    "def average_timedelta(timedeltas: list[timedelta]):\n",
    "    return sum(timedeltas, timedelta()) / len(timedeltas)\n",
    "\n",
    "\n",
    "def measure_inference_time(prompt: str, num_iterations: int = DEFAULT_NO_ITERATIONS, model_id: str = DEFAULT_MODEL_ID, region_name: str = DEFAULT_REGION, latency_optimized: bool = False, print_output: bool = False):\n",
    "    inference_times = []\n",
    "    tokens = []\n",
    "    for i in range(num_iterations):\n",
    "        response, duration, total_tokens = send_prompt_to_bedrock(prompt=prompt, model_id=model_id, region_name=region_name, latency_optimized=latency_optimized)\n",
    "        if duration:\n",
    "            inference_times.append(duration)\n",
    "            tokens.append(total_tokens)\n",
    "            logger.debug(f\"Iteration {i+1}: {duration} seconds processing {total_tokens} tokens.\")\n",
    "            # Optional: Print the model's response\n",
    "            if response and 'output' in response:\n",
    "                logger.debug(f\"Model response: {response['output']['message']['content'][0]['text']}\")\n",
    "            if print_output:\n",
    "                print(f\"Performance in run {i+1}: {total_tokens} tokens in {duration.total_seconds()} seconds equals {total_tokens/duration.total_seconds()} tokens/sec\")\n",
    "    return inference_times, tokens\n",
    "\n",
    "\n",
    "def print_results(inference_times: list[timedelta], model_id: str, latency_optimized: bool, region_name: str, num_iterations: int, tokens: list[int], prompt: str):\n",
    "    if latency_optimized:\n",
    "        header = f\"Results over {num_iterations} iterations with **Latency Optimized Performance** using {model_id} in {region_name}:\"\n",
    "    else:\n",
    "        header = f\"Results over {num_iterations} iterations with **Standard Performance** using {model_id} in {region_name}:\"\n",
    "\n",
    "    sum_tokens = sum(tokens)\n",
    "    avg_tokens = sum_tokens / num_iterations\n",
    "    min_tokens = min(tokens)\n",
    "    max_tokens = max(tokens)\n",
    "    tokens_per_second = calculate_tokens_per_sec(timedeltas=inference_times, tokens=tokens)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    {header}\n",
    "        Prompt: {prompt}\n",
    "        Avg./Min./Max. Tokens per response: {avg_tokens} / {min_tokens} / {max_tokens} tokens\n",
    "        Total Tokens generated: {sum_tokens}\n",
    "        Average Inference Time: {average_timedelta(timedeltas=inference_times)} seconds\n",
    "        Median Inference Time: {statistics.median(inference_times)} seconds\n",
    "        Minimum Inference Time: {min(inference_times)} seconds\n",
    "        Maximum Inference Time: {max(inference_times)} seconds\n",
    "        Performance: {tokens_per_second} tokens / sec\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "def calculate_tokens_per_sec(timedeltas: list[timedelta], tokens: list[int]) -> float:\n",
    "    seconds = 0\n",
    "    token_count = 0\n",
    "    for delta_obj in timedeltas:\n",
    "        seconds += int(delta_obj.total_seconds())\n",
    "    for t in tokens:\n",
    "        token_count += t\n",
    "\n",
    "    return token_count / seconds\n",
    "\n",
    "\n",
    "def run_comparison(prompt: str, num_iterations: int = DEFAULT_NO_ITERATIONS, model_id: str = DEFAULT_MODEL_ID, region_name: str = DEFAULT_REGION, input_prompt_prefix: str = \"\", print_output: bool = False):\n",
    "    latency_optimized = False\n",
    "    prompt = (input_prompt_prefix.strip() + \" \" + prompt).strip()\n",
    "    without_optimized_inference, tokens_without_optimized_inference = measure_inference_time(\n",
    "        prompt=prompt,\n",
    "        num_iterations=num_iterations,\n",
    "        model_id=model_id,\n",
    "        region_name=region_name,\n",
    "        latency_optimized=latency_optimized,\n",
    "        print_output=print_output\n",
    "    )\n",
    "    # Calculate and display results\n",
    "    if without_optimized_inference:\n",
    "        print_results(\n",
    "            inference_times=without_optimized_inference,\n",
    "            model_id=model_id,\n",
    "            latency_optimized=latency_optimized,\n",
    "            region_name=region_name,\n",
    "            num_iterations=num_iterations,\n",
    "            tokens=tokens_without_optimized_inference,\n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "    latency_optimized = True\n",
    "    with_optimized_inference, tokens_with_optimized_inference = measure_inference_time(\n",
    "        prompt=prompt,\n",
    "        num_iterations=num_iterations,\n",
    "        model_id=model_id,\n",
    "        region_name=region_name,\n",
    "        latency_optimized=latency_optimized,\n",
    "        print_output=print_output\n",
    "    )\n",
    "    # Calculate and display results\n",
    "    if with_optimized_inference:\n",
    "        print_results(\n",
    "            inference_times=with_optimized_inference,\n",
    "            model_id=model_id,\n",
    "            latency_optimized=latency_optimized,\n",
    "            region_name=region_name,\n",
    "            num_iterations=num_iterations,\n",
    "            tokens=tokens_with_optimized_inference,\n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "    return with_optimized_inference, without_optimized_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d8ab4-52ba-4377-a784-2edd653f725f",
   "metadata": {},
   "source": [
    "# Test Execution\n",
    "The cell below executes the test. Make sure that all of the cells above have been executed before executing this cell. Depending on the prompt that is used to do this evaluation, the complete execution of this cell can take a few minutes. \n",
    "**Important**: Depending on the quota of the AWS account for which Bedrock is accessed, running this cell may cause throttling of Bedrock. The code above handles these exceptions by waiting, but in such cases, the execution of the cell may take longer. Note that these waiting times are not included in the performance measurement.\n",
    "\n",
    "\n",
    "Finally, a result like the following will appear:\n",
    "~~~\n",
    "    Results over 25 iterations with **Standard Performance** using us.meta.llama3-1-405b-instruct-v1:0 in us-east-2:\n",
    "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
    "        Avg./Min./Max. Tokens per response: 883.88 / 736 / 1029 tokens\n",
    "        Total Tokens generated: 22097\n",
    "        Average Inference Time: 0:00:53.573597 seconds\n",
    "        Median Inference Time: 0:00:53.170353 seconds\n",
    "        Minimum Inference Time: 0:00:44.332831 seconds\n",
    "        Maximum Inference Time: 0:01:02.987788 seconds\n",
    "        Performance: 16.639307228915662 tokens / sec\n",
    "        \n",
    "\n",
    "    Results over 25 iterations with **Latency Optimized Performance** using us.meta.llama3-1-405b-instruct-v1:0 in us-east-2:\n",
    "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
    "        Avg./Min./Max. Tokens per response: 897.84 / 782 / 1046 tokens\n",
    "        Total Tokens generated: 22446\n",
    "        Average Inference Time: 0:00:13.992810 seconds\n",
    "        Median Inference Time: 0:00:13.551137 seconds\n",
    "        Minimum Inference Time: 0:00:11.460454 seconds\n",
    "        Maximum Inference Time: 0:00:19.525074 seconds\n",
    "        Performance: 66.6053412462908 tokens / sec\n",
    "~~~\n",
    "\n",
    "The two blocks of information contain all the required information including the model that was used (in the example above that is `us.meta.llama3-1-405b-instruct-v1:0`). The values below this header line provide the average, median, minimum and maximum latency over the complete execution of the prompt in Amazon Bedrock as well as the average number of tokens that have been generated per second. The latency in this context is the time it takes from sending the prompt to receiving the response from the Bedrock service. For instance, in the example above, the average time it took Bedrock to generate a response *without* optimized inference was almost 54 seconds, while the optimized inferences had about 14 seconds for the same task. Similarly, the performance is the sum of the total number of generated tokens divided by the total number of seconds of latency. In the example above, optimized inference generated more than 66 tokens per second, while the non-optimized version only generated less than 17 tokens, i.e., the optimized inference was better by a factor of approximately 4x compared to the non-optimized inference.\n",
    "\n",
    "Findings so far show that the more output a prompt creates and the large the used LLM is, the higher is also the impact of the optimized performance. For instance, the simpler prompt with Claude Haiku 3.5 only requires 2.5 seconds on average even without optimized inference and just 1.4 seconds with optimized inference. Hence, the factor is less than 2x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16998b2f-5c12-4aef-aa65-d95b3ce01811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Results over 25 iterations with **Standard Performance** using us.meta.llama3-1-405b-instruct-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 883.88 / 736 / 1029 tokens\n",
      "        Total Tokens generated: 22097\n",
      "        Average Inference Time: 0:00:53.573597 seconds\n",
      "        Median Inference Time: 0:00:53.170353 seconds\n",
      "        Minimum Inference Time: 0:00:44.332831 seconds\n",
      "        Maximum Inference Time: 0:01:02.987788 seconds\n",
      "        Performance: 16.639307228915662 tokens / sec\n",
      "        \n",
      "\n",
      "    Results over 25 iterations with **Latency Optimized Performance** using us.meta.llama3-1-405b-instruct-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 897.84 / 782 / 1046 tokens\n",
      "        Total Tokens generated: 22446\n",
      "        Average Inference Time: 0:00:13.992810 seconds\n",
      "        Median Inference Time: 0:00:13.551137 seconds\n",
      "        Minimum Inference Time: 0:00:11.460454 seconds\n",
      "        Maximum Inference Time: 0:00:19.525074 seconds\n",
      "        Performance: 66.6053412462908 tokens / sec\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "optimized_inference_result, non_optimized_inference_result = run_comparison(\n",
    "    prompt=prompt,\n",
    "    num_iterations=DEFAULT_NO_ITERATIONS,\n",
    "    model_id=MODEL_ID_META_LLAMA_3_1_405B_INSTRUCT,\n",
    "    region_name=DEFAULT_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffffec8-34c4-4114-b9be-9e0fbad7c5c0",
   "metadata": {},
   "source": [
    "# Anthropic Claude Haiku 3.5\n",
    "This section provides code to produce performance values Anthropic's Haiku 3.5 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c9ff07-1dd6-4351-af82-cb4fc9cc9b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Results over 25 iterations with **Standard Performance** using us.anthropic.claude-3-5-haiku-20241022-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 776.32 / 694 / 865 tokens\n",
      "        Total Tokens generated: 19408\n",
      "        Average Inference Time: 0:00:24.696465 seconds\n",
      "        Median Inference Time: 0:00:24.425361 seconds\n",
      "        Minimum Inference Time: 0:00:19.212240 seconds\n",
      "        Maximum Inference Time: 0:00:30.940138 seconds\n",
      "        Performance: 32.079338842975204 tokens / sec\n",
      "        \n",
      "\n",
      "    Results over 25 iterations with **Latency Optimized Performance** using us.anthropic.claude-3-5-haiku-20241022-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 759.64 / 561 / 811 tokens\n",
      "        Total Tokens generated: 18991\n",
      "        Average Inference Time: 0:00:11.651419 seconds\n",
      "        Median Inference Time: 0:00:11.230326 seconds\n",
      "        Minimum Inference Time: 0:00:07.912783 seconds\n",
      "        Maximum Inference Time: 0:00:17.900541 seconds\n",
      "        Performance: 68.31294964028777 tokens / sec\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "optimized_inference_result, non_optimized_inference_result = run_comparison(\n",
    "    prompt=prompt,\n",
    "    num_iterations=DEFAULT_NO_ITERATIONS,\n",
    "    model_id=MODEL_ID_ANTHROPIC_CLAUDE3_5_HAIKU_CRIS,\n",
    "    region_name=\"us-east-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a243d95-c4ef-464a-a2b6-0c7e59c4c3fb",
   "metadata": {},
   "source": [
    "# Meta Llama 3.1 70B\n",
    "As witht he above section, this section uses the smaller version of the Llama 3.1 LLM with 70 billion parameters for the inference. It is particularly interesting to compare these values with the efficiency gains obtained for the 405B version of this model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83bd9f97-d3e0-4ee8-b1ca-a9f00c2265bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Results over 25 iterations with **Standard Performance** using us.meta.llama3-1-70b-instruct-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 937.24 / 842 / 1079 tokens\n",
      "        Total Tokens generated: 23431\n",
      "        Average Inference Time: 0:00:29.065641 seconds\n",
      "        Median Inference Time: 0:00:28.842485 seconds\n",
      "        Minimum Inference Time: 0:00:26.658636 seconds\n",
      "        Maximum Inference Time: 0:00:33.336058 seconds\n",
      "        Performance: 32.862552594670404 tokens / sec\n",
      "        \n",
      "\n",
      "    Results over 25 iterations with **Latency Optimized Performance** using us.meta.llama3-1-70b-instruct-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 1035.76 / 880 / 1213 tokens\n",
      "        Total Tokens generated: 25894\n",
      "        Average Inference Time: 0:00:08.386833 seconds\n",
      "        Median Inference Time: 0:00:07.973159 seconds\n",
      "        Minimum Inference Time: 0:00:06.593050 seconds\n",
      "        Maximum Inference Time: 0:00:12.586957 seconds\n",
      "        Performance: 132.1122448979592 tokens / sec\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "optimized_inference_result, non_optimized_inference_result = run_comparison(\n",
    "    prompt=prompt,\n",
    "    num_iterations=DEFAULT_NO_ITERATIONS,\n",
    "    model_id=MODEL_ID_META_LLAMA_3_1_70B_INSTRUCT,\n",
    "    region_name=\"us-east-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f847f4-c1ee-4821-b11a-050db011e491",
   "metadata": {},
   "source": [
    "# Meta Llama 3.1 405B\n",
    "This section provides performance metrics for Llama 405B, i.e., the largest version (405 billion parameters) of the Llama 3.1 LLMs from Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "852df472-0091-4cf9-9fc1-005af734b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Results over 25 iterations with **Standard Performance** using us.meta.llama3-1-405b-instruct-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 872.28 / 756 / 1034 tokens\n",
      "        Total Tokens generated: 21807\n",
      "        Average Inference Time: 0:00:52.804474 seconds\n",
      "        Median Inference Time: 0:00:50.241769 seconds\n",
      "        Minimum Inference Time: 0:00:45.679469 seconds\n",
      "        Maximum Inference Time: 0:01:02.850421 seconds\n",
      "        Performance: 16.65928189457601 tokens / sec\n",
      "        \n",
      "\n",
      "    Results over 25 iterations with **Latency Optimized Performance** using us.meta.llama3-1-405b-instruct-v1:0 in us-east-2:\n",
      "        Prompt: Write a book chapter on why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 901.0 / 775 / 1040 tokens\n",
      "        Total Tokens generated: 22525\n",
      "        Average Inference Time: 0:00:16.397900 seconds\n",
      "        Median Inference Time: 0:00:13.359696 seconds\n",
      "        Minimum Inference Time: 0:00:11.745071 seconds\n",
      "        Maximum Inference Time: 0:00:50.793698 seconds\n",
      "        Performance: 56.88131313131313 tokens / sec\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "optimized_inference_result, non_optimized_inference_result = run_comparison(\n",
    "    prompt=prompt,\n",
    "    num_iterations=DEFAULT_NO_ITERATIONS,\n",
    "    model_id=MODEL_ID_META_LLAMA_3_1_405B_INSTRUCT,\n",
    "    region_name=\"us-east-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be038b7e-cc11-4266-8ed5-84496835ea1f",
   "metadata": {},
   "source": [
    "# Amazon Nova Pro\n",
    "As with the previous sections, this section provides performance comparison numbers for Amazon Nova Pro. Note that at the time of writing this notebook, this feature along with Nova Pro was in preview and only available in us-east-1 through cross-region inference (CRIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf2dfa6-887b-4214-9be4-7ca5fba98a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Results over 25 iterations with **Standard Performance** using us.amazon.nova-pro-v1:0 in us-east-1:\n",
      "        Prompt: Explain in a few sentences why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 92.8 / 87 / 97 tokens\n",
      "        Total Tokens generated: 2320\n",
      "        Average Inference Time: 0:00:01.562418 seconds\n",
      "        Median Inference Time: 0:00:01.105986 seconds\n",
      "        Minimum Inference Time: 0:00:00.978808 seconds\n",
      "        Maximum Inference Time: 0:00:06.003716 seconds\n",
      "        Performance: 72.5 tokens / sec\n",
      "        \n",
      "\n",
      "    Results over 25 iterations with **Latency Optimized Performance** using us.amazon.nova-pro-v1:0 in us-east-1:\n",
      "        Prompt: Explain in a few sentences why objects cannot travel faster than the speed of light.\n",
      "        Avg./Min./Max. Tokens per response: 87.88 / 87 / 88 tokens\n",
      "        Total Tokens generated: 2197\n",
      "        Average Inference Time: 0:00:01.232627 seconds\n",
      "        Median Inference Time: 0:00:01.068637 seconds\n",
      "        Minimum Inference Time: 0:00:01.000697 seconds\n",
      "        Maximum Inference Time: 0:00:05.084587 seconds\n",
      "        Performance: 75.75862068965517 tokens / sec\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "optimized_inference_result, non_optimized_inference_result = run_comparison(\n",
    "    prompt=simple_prompt,\n",
    "    num_iterations=DEFAULT_NO_ITERATIONS,\n",
    "    model_id=MODEL_ID_AMAZON_NOVA_PRO_CRIS,\n",
    "    region_name=\"us-east-1\",\n",
    "    #input_prompt_prefix=text_for_input_prompt_lengthening\n",
    "    print_output=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
