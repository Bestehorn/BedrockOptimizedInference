{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6476a48e-3b7c-4d11-903c-b24af363d971",
   "metadata": {},
   "source": [
    "# Bedrock Optimized Latency Test (by Markus Bestehorn)\n",
    "This notebook contains code to test the optimized performance inference feature of Bedrock that has been released for public preview at re:invent 2024: https://aws.amazon.com/about-aws/whats-new/2024/12/latency-optimized-inference-foundation-models-amazon-bedrock/\n",
    "\n",
    "The code allows comparing the inference result times of the standard inference with the inference times for the latency optimized inference. **Running this test will incur costs on the AWS account running it**. For further information on the cost - particularly on optimized inference - refer to the official pricing documentation page of Amazon Bedrock: https://aws.amazon.com/bedrock/pricing/  \n",
    "\n",
    "**Disclaimer**: The code in this notebook has been written for the sole purpose of testing the aforementioned feature of Amazon Bedrock. This code is not production ready or usable for other purposes.\n",
    "\n",
    "**Prerequisites**: \n",
    "1. It is assumed here, that the this notebook runs inside a security context that has adequate priviledges to use the converse API of Amazon Bedrock.\n",
    "2. It is assumed that model access has been adequately configured in Amazon Bedrock through the \"Model Access\" page for the region that will be used (\"us-east-2\") as well as all foundational models that will be used.\n",
    "\n",
    "As a first step, we need to make sure that the most recent version of the boto3 library is installed where this notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2ff8e-8b87-4abf-bdf3-bbfcdc87b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def update_boto3():\n",
    "    # Implement pip upgrade using subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'boto3'])\n",
    "    \n",
    "    # Verify the new version\n",
    "    import boto3\n",
    "    print(f\"Current boto3 version: {boto3.__version__}\")\n",
    "\n",
    "update_boto3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ca3d0-d8b5-4805-8027-0487d8e8dcbc",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "The test uses the Bedrock converse API to respond to the same prompt multiple times and then computes aggregates over the times required to complete these calls. The main parameters for this test are as follows:\n",
    " -  `prompt`: This is the prompt that is being sent to the Bedrock converse API. This can be modified freely and depending on the complexity of the prompt, the absolute values of the test may vary.\n",
    " -  `DEFAULT_MODEL_ID`: This is the model ID or inference profile that is used for the test. As optimized inference is currently only supported by a limited number of foundational models in Bedrock, it is not recommended to change this value.\n",
    " -  `DEFAULT_REGION`: This is the AWS region where the inference will be running. Do not change this, as optimized performance is currently not supported in other AWS regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b508f40-004b-4290-9d0b-d6b968694f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some predefined prompts that result in a different number of output tokens and therefore different processing times in Bedrock\n",
    "simple_prompt = \"\"\"Explain in a few sentences why objects cannot travel faster than the speed of light.\"\"\"\n",
    "complex_prompt = \"\"\"Write a book chapter on why objects cannot travel faster than the speed of light.\"\"\"\n",
    "\n",
    "# The variable that actually defined which prompt is being used\n",
    "prompt = simple_prompt\n",
    "DEFAULT_NO_ITERATIONS = 5\n",
    "DEFAULT_REGION = \"us-east-2\" # do not change this value => optimized performance is currently only available in us-east-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fff82-3540-4427-bbbc-b656d288560d",
   "metadata": {},
   "source": [
    "## Foundational Model\n",
    "Optimized performance is currently only supported for a limited number of foundational models as documented here: https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html\n",
    "\n",
    "The cell below configures the model ID that will be used in a variable called `DEFAULT_MODEL_ID`. If you want to change the model that is used here, use one of the other provided Model IDs and copy them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46c28681-69ab-4eb5-9603-f9d0c5cc8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID_ANTHROPIC_CLAUDE3_5_HAIKU_CRIS = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "MODEL_ID_META_LLAMA_3_1_70B_INSTRUCT = \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "MODEL_ID_META_LLAMA_3_1_405B_INSTRUCT = \"us.meta.llama3-1-405b-instruct-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL_ID = MODEL_ID_ANTHROPIC_CLAUDE3_5_HAIKU_CRIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33c472-1575-4254-ade2-69e388354530",
   "metadata": {},
   "source": [
    "## Code\n",
    "The following cell contains all code that is used for running the test. Note that executing the cell does not actually execute the code, but merely loads it so that it can be executed when needed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5023c053-8092-43fa-8664-9353e7ef4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from statistics import mean\n",
    "import os\n",
    "from botocore.config import Config\n",
    "from datetime import datetime, timedelta\n",
    "from botocore.exceptions import ClientError\n",
    "import statistics\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "BEDROCK_THROTTLING_PAUSE = 30\n",
    "\n",
    "\n",
    "def json_to_str(input_json: dict) -> str:\n",
    "    return json.dumps(input_json, indent=3)\n",
    "\n",
    "\n",
    "def send_prompt_to_bedrock(prompt: str,\n",
    "                          additional_files: list[dict] = [],\n",
    "                          model_id: str = DEFAULT_MODEL_ID, \n",
    "                          max_tokens: int = 4096,\n",
    "                          temperature: float = 0.7,\n",
    "                          top_p: float = 0.9,\n",
    "                          api_key: str = None, \n",
    "                          region_name: str = DEFAULT_REGION,\n",
    "                          latency_optimized=False) -> (dict, float):\n",
    "    config = Config(\n",
    "        read_timeout=240,  # Timeout in seconds\n",
    "        connect_timeout=60,  # Connection timeout\n",
    "        retries={'max_attempts': 3}  # Optional: Configure retry behavior\n",
    "    )\n",
    "\n",
    "    # Create a Bedrock client\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime', region_name=region_name, config=config)\n",
    "\n",
    "    if prompt is None or len(prompt) == 0:\n",
    "        logger.error(\"Prompt cannot be empty.\")\n",
    "        return None\n",
    "\n",
    "    content_blocks = [\n",
    "        {\"text\": prompt}\n",
    "    ]\n",
    "\n",
    "    for current_doc_element in additional_files:\n",
    "        content_blocks.append({\"document\": current_doc_element})\n",
    "\n",
    "    # setup the performance config\n",
    "    performanceConfig = {}\n",
    "    if latency_optimized:\n",
    "        performanceConfig[\"latency\"] = \"optimized\"\n",
    "    else:\n",
    "        performanceConfig[\"latency\"] = \"standard\"\n",
    "    \n",
    "    # we loop here to retry in case of throttling\n",
    "    done = False\n",
    "    tries = 0\n",
    "    last_toggle_was_region = False\n",
    "    while not done:\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            tries += 1\n",
    "            logger.debug(f\"Sending prompt with the following performance config to Bedrock:\\n{json_to_str(performanceConfig)}\")\n",
    "            start_time = datetime.now()\n",
    "            response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content_blocks\n",
    "                    }\n",
    "                ],\n",
    "                inferenceConfig={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"topP\": top_p,\n",
    "                    \"maxTokens\": max_tokens\n",
    "                },\n",
    "                performanceConfig=performanceConfig\n",
    "            )\n",
    "            end_time = datetime.now()\n",
    "            elapsed_time = end_time - start_time\n",
    "            logger.debug(f\"Measured time: {elapsed_time} (Type: {elapsed_time.__class__})\")\n",
    "            total_tokens = int(response[\"usage\"][\"totalTokens\"])\n",
    "            done = True\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"ThrottlingException\":\n",
    "                pause = BEDROCK_THROTTLING_PAUSE * tries\n",
    "                logger.warning(f\"Call to Bedrock was throttled. Waiting for {pause} seconds before retrying:\\n{str(e)}\")\n",
    "                time.sleep(pause)\n",
    "                continue\n",
    "            elif e.response['Error']['Code'] == \"ServiceUnavailableException\":\n",
    "                pause = BEDROCK_THROTTLING_PAUSE * tries\n",
    "                logger.warning(f\"Bedrock services is unavailable at the moment. Waiting for {pause} seconds before retrying:\\n{str(e)}\")\n",
    "                time.sleep(pause)\n",
    "                continue\n",
    "            else:\n",
    "                logger.error(f\"Can't invoke {model_id} due to Client Error. Reason: {str(e)}\\nType: {str(e.__class__)}\")\n",
    "                continue\n",
    "        except ParamValidationError as pe:\n",
    "            logger.error(f\"Cannot invoke model with ID {model_id} as the format of the parameters in the input is illegal:\\n{str(pe)}\\nError Type: {e.__class__}\\nAborting call to bedrock.\")\n",
    "            break\n",
    "        except TypeError as te:\n",
    "            logger.error(f\"Cannot invoke model with ID {model_id} as the call to the converse API is malformatted/illegal:\\n{str(te)}\\nError Type: {te.__class__}\\nAborting call to bedrock.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Call to Bedrock caused an Exception: {str(e)} \\nType: {str(e.__class__)}\")\n",
    "            break\n",
    "\n",
    "    logger.debug(f\"Received the following response from Bedrock:\\n{json_to_str(response)}\\nReponse Type: {response.__class__}\")\n",
    "\n",
    "    return response, elapsed_time, total_tokens\n",
    "\n",
    "\n",
    "def average_timedelta(timedeltas: list[timedelta]):\n",
    "    return sum(timedeltas, timedelta()) / len(timedeltas)\n",
    "\n",
    "\n",
    "def measure_inference_time(prompt: str, num_iterations: int = DEFAULT_NO_ITERATIONS, model_id: str = DEFAULT_MODEL_ID, region_name: str = DEFAULT_REGION, latency_optimized: bool = False):\n",
    "    inference_times = []\n",
    "    tokens = []\n",
    "    for i in range(num_iterations):\n",
    "        response, duration, total_tokens = send_prompt_to_bedrock(prompt=prompt, model_id=model_id, region_name=region_name, latency_optimized=latency_optimized)\n",
    "        if duration:\n",
    "            inference_times.append(duration)\n",
    "            tokens.append(total_tokens)\n",
    "            logger.debug(f\"Iteration {i+1}: {duration} seconds processing {total_tokens} tokens.\")\n",
    "            # Optional: Print the model's response\n",
    "            if response and 'output' in response:\n",
    "                logger.debug(f\"Model response: {response['output']['message']['content'][0]['text']}\")\n",
    "    return inference_times, tokens\n",
    "\n",
    "\n",
    "def print_results(inference_times: list[timedelta], model_id: str, latency_optimized: bool, tokens_per_second: float):\n",
    "    if latency_optimized:\n",
    "        header = f\"Results with Latency Optimized Performance using {model_id}:\"\n",
    "    else:\n",
    "        header = f\"Results with Standard Performance using {model_id}:\"\n",
    "\n",
    "    print(f\"\"\"\n",
    "        {header}\n",
    "            Average Inference Time: {average_timedelta(timedeltas=inference_times)} seconds\n",
    "            Median Inference Time: {statistics.median(inference_times)} seconds\n",
    "            Minimum Inference Time: {min(inference_times)} seconds\n",
    "            Maximum Inference Time: {max(inference_times)} seconds\n",
    "            Performance: {tokens_per_second} tokens / sec\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "def calculate_tokens_per_sec(timedeltas: list[timedelta], tokens: list[int]) -> float:\n",
    "    seconds = 0\n",
    "    token_count = 0\n",
    "    for delta_obj in timedeltas:\n",
    "        seconds += int(delta_obj.total_seconds())\n",
    "    for t in tokens:\n",
    "        token_count += t\n",
    "\n",
    "    return token_count / seconds\n",
    "\n",
    "\n",
    "def run_comparison(prompt: str, num_iterations: int = DEFAULT_NO_ITERATIONS, model_id: str = DEFAULT_MODEL_ID, region_name: str = DEFAULT_REGION):\n",
    "    latency_optimized = False\n",
    "    without_optimized_inference, tokens_without_optimized_inference = measure_inference_time(\n",
    "        prompt=prompt,\n",
    "        num_iterations=num_iterations,\n",
    "        model_id=model_id,\n",
    "        region_name=region_name,\n",
    "        latency_optimized=latency_optimized\n",
    "    )\n",
    "    # Calculate and display results\n",
    "    if without_optimized_inference:\n",
    "        print_results(inference_times=without_optimized_inference, model_id=model_id, latency_optimized=latency_optimized, tokens_per_second=calculate_tokens_per_sec(timedeltas=without_optimized_inference, tokens=tokens_without_optimized_inference))\n",
    "\n",
    "    latency_optimized = True\n",
    "    with_optimized_inference, tokens_with_optimized_inference = measure_inference_time(\n",
    "        prompt=prompt,\n",
    "        num_iterations=num_iterations,\n",
    "        model_id=model_id,\n",
    "        region_name=region_name,\n",
    "        latency_optimized=latency_optimized\n",
    "    )\n",
    "    # Calculate and display results\n",
    "    if with_optimized_inference:\n",
    "        print_results(inference_times=with_optimized_inference, model_id=model_id, latency_optimized=latency_optimized, tokens_per_second=calculate_tokens_per_sec(timedeltas=with_optimized_inference, tokens=tokens_with_optimized_inference))\n",
    "\n",
    "    return with_optimized_inference, with_optimized_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d8ab4-52ba-4377-a784-2edd653f725f",
   "metadata": {},
   "source": [
    "# Test Execution\n",
    "The cell below executes the test. Make sure that all of the cells above have been executed before executing this cell. Depending on the prompt that is used to do this evaluation, the complete execution of this cell can take a few minutes. \n",
    "**Important**: Depending on the quota of the AWS account for which Bedrock is accessed, running this cell may cause throttling of Bedrock. The code above handles these exceptions by waiting, but in such cases, the execution of the cell may take longer. \n",
    "\n",
    "\n",
    "Finally, a result like the following will appear:\n",
    "~~~\n",
    "Results with Standard Performance using us.meta.llama3-1-405b-instruct-v1:0:\n",
    "    Average Inference Time: 0:00:54.931572 seconds\n",
    "    Median Inference Time: 0:00:55.680593 seconds\n",
    "    Minimum Inference Time: 0:00:49.521402 seconds\n",
    "    Maximum Inference Time: 0:01:01.527845 seconds\n",
    "    Performance: 16.63235294117647 tokens / sec\n",
    "Results with Latency Optimized Performance using us.meta.llama3-1-405b-instruct-v1:0:\n",
    "    Average Inference Time: 0:00:14.460839 seconds\n",
    "    Median Inference Time: 0:00:14.165186 seconds\n",
    "    Minimum Inference Time: 0:00:12.774470 seconds\n",
    "    Maximum Inference Time: 0:00:17.149608 seconds\n",
    "    Performance: 64.4 tokens / sec\n",
    "~~~\n",
    "\n",
    "The two blocks of information contain all the required information including the model that was used (in the example above that is `us.meta.llama3-1-405b-instruct-v1:0`). The values below this header line provide the average, median, minimum and maximum latency over the complete execution of the prompt in Amazon Bedrock as well as the average number of tokens that have been generated per second. The latency in this context is the time it takes from sending the prompt to receiving the response from the Bedrock service. For instance, in the example above, the average time it took Bedrock to generate a response *without* optimized inference was almost 55 seconds, while the optimized inferences had about 14.5 seconds for the same task. Similarly, the performance is the sum of the total number of generated tokens divided by the total number of seconds of latency. In the example above, optimized inference generated more than 64 tokens per second, while the non-optimized version only generated less than 17 tokens, i.e., the optimized inference was better by a factor of 3.7x compared to the non-optimized inference.\n",
    "\n",
    "Findings so far show that the more output a prompt creates and the large the used LLM is, the higher is also the impact of the optimized performance. For instance, the simpler prompt with Claude Haiku 3.5 only requires 2.5 seconds on average even without optimized inference and just 1.4 seconds with optimized inference. Hence, the factor is only 2x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16998b2f-5c12-4aef-aa65-d95b3ce01811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Results with Standard Performance using us.anthropic.claude-3-5-haiku-20241022-v1:0:\n",
      "            Average Inference Time: 0:00:02.526479 seconds\n",
      "            Median Inference Time: 0:00:02.397623 seconds\n",
      "            Minimum Inference Time: 0:00:02.194138 seconds\n",
      "            Maximum Inference Time: 0:00:02.833778 seconds\n",
      "            Performance: 56.0 tokens / sec\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Call to Bedrock was throttled. Waiting for 30 seconds before retrying:\n",
      "An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 3): Too many requests, please wait before trying again.\n",
      "WARNING:__main__:Call to Bedrock was throttled. Waiting for 30 seconds before retrying:\n",
      "An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 3): Too many requests, please wait before trying again.\n",
      "WARNING:__main__:Call to Bedrock was throttled. Waiting for 30 seconds before retrying:\n",
      "An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 3): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Results with Latency Optimized Performance using us.anthropic.claude-3-5-haiku-20241022-v1:0:\n",
      "            Average Inference Time: 0:00:01.417591 seconds\n",
      "            Median Inference Time: 0:00:01.405501 seconds\n",
      "            Minimum Inference Time: 0:00:01.198176 seconds\n",
      "            Maximum Inference Time: 0:00:01.617868 seconds\n",
      "            Performance: 114.8 tokens / sec\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "with_optimized_inference, with_optimized_inference = run_comparison(\n",
    "    prompt=prompt,\n",
    "    num_iterations=DEFAULT_NO_ITERATIONS,\n",
    "    model_id=MODEL_ID_ANTHROPIC_CLAUDE3_5_HAIKU_CRIS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
